# -*- coding: utf-8 -*-
"""Dataloaders

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ij2bcl42VjEl-Cjlg-cViHKA2v_juT9L
"""

import warnings
warnings.filterwarnings('ignore')

import os

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

import numpy as np
import pandas as pd
import missingno as msno

import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

from tqdm.notebook import tqdm
from glob import glob

seed = 42
pd.set_option('display.max_colwidth', None)

def seed_everything(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything()

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Currently using "{device.upper()}" device.')

"""#### External data EDA"""

df = pd.read_csv("/kaggle/input/isic-2019/ISIC_2019_Training_Metadata.csv").drop("lesion_id", axis=1)
im_df = pd.read_csv("/kaggle/input/isic-2019/ISIC_2019_Training_GroundTruth.csv").drop("UNK", axis=1)
im_df["labels"] = im_df.iloc[:, 1:].idxmax(axis=1)

df = df.join(im_df.set_index("image"), on=["image"]).drop(["MEL", "NV", "BCC", "AK", "DF", "VASC", "SCC", "BKL"], axis=1)
df = df[~df["image"].str.contains("downsampled")]

df.head()

msno.matrix(df, figsize=(10,6))

msno.heatmap(df, figsize=(10,6))

plt.figure(figsize=(10,6))
sns.histplot(x="age_approx", hue="labels", data=df, kde=True)
plt.show()

plt.figure(figsize=(14,6))
sns.countplot(x="anatom_site_general", hue="labels", data=df)
plt.legend(loc="upper right")
plt.show()

import plotly.express as px

fig = px.histogram(df, x="labels", nbins=8, width=800, height=400)
fig.show()

df["age_approx"] = df["age_approx"].fillna(df["age_approx"].mean()).astype("int")
df["anatom_site_general"] = df["anatom_site_general"].fillna("unknown")
df["sex"] = df["sex"].fillna("unknown")

df["age_bins"] = pd.cut(df["age_approx"], bins=[-1, 20, 40, 60, 80, 95], labels=False)

labels = {"NV": 0, "MEL": 1, "BCC": 1, "BKL": 0, "AK": 0, "SCC": 1, "VASC": 0, "DF": 0}

from sklearn.preprocessing import StandardScaler

X = df[["age_approx", "anatom_site_general", "sex", "labels"]]
X = pd.get_dummies(X, columns=["anatom_site_general", "sex"], drop_first=True)
X["labels"] = X["labels"].map(labels)

X, y = X.drop("labels", axis=1), X["labels"]

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y, random_state=seed)

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

transformer = ColumnTransformer(transformers=[("scaler", StandardScaler(), ["age_approx",]),], remainder="passthrough")

pipe = make_pipeline(transformer,
                     RandomForestClassifier(n_estimators=100, max_depth=15)
                    ).fit(x_train, y_train)

y_pred = pipe.predict(x_test)

print(classification_report(y_test, y_pred))

# activation function: Swish

class Swish(nn.Module):
    """
    activation function:
    Swish allows a small number of negative weights to be propagated through,
    while ReLU (max(0, x)) thresholds all negative weights to zero.
    """
    def __init__(self, *args, **kwargs):
        super(Swish, self).__init__(*args, **kwargs)

    def forward(self, x):
        return x * torch.sigmoid(x)


class ConvBNBlock(nn.Module):
    """
    basic block: zero-padded 2D convolution, followed by batch
    normalization and Swish activation
    """
    def __init__(self, in_channels, out_channels, kernel_size, *args, stride=1, groups=1, **kwargs):
        super(ConvBNBlock, self).__init__(*args, **kwargs)
        padding = self._get_padding(kernel_size, stride)
        self.block = nn.Sequential(
                nn.ZeroPad2d(padding),
                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=0, groups=groups, bias=False),
                nn.BatchNorm2d(out_channels),
                Swish(),
            )

    def forward(self, x):
        return self.block(x)

    def _get_padding(self, kernel_size, stride):
        """ add corresponding padding """
        p = np.maximum(kernel_size - stride, 0)
        return [p // 2, p - p // 2, p // 2, p - p // 2]


class SqueezeExcitationBlock(nn.Module):
    """
    The Squeeze-and-Excitation Block is an architectural unit designed to improve
    the representational power of a network by enabling it to perform
    dynamic channel-wise feature recalibration
    """
    def __init__(self, in_channels, reduced_dim, *args, **kwargs):
        super(SqueezeExcitationBlock, self).__init__(*args, **kwargs)
        self.squeeze_excitation = nn.Sequential(
                    nn.AdaptiveAvgPool2d(1),
                    nn.Conv2d(in_channels, reduced_dim, kernel_size=1),
                    Swish(),
                    nn.Conv2d(reduced_dim, in_channels, kernel_size=1),
                    nn.Sigmoid()
            )

    def forward(self, x):
        return x * self.squeeze_excitation(x)  # which is similar to swish activation


class MBConvBlock(nn.Module):
    """
    Inverted Linear BottleNeck layer with Depth-Wise Separable Convolution
    implements inverted residual connection like MobileNetV2
    """
    def __init__(self, in_channels, out_channels, expand_ratio, kernel_size, stride,
                 *args, reduction_ratio=0.4, drop_connect_rate=0.2, **kwargs):
        super(MBConvBlock, self).__init__(*args, **kwargs)

        self.drop_connect_rate = drop_connect_rate
        self.use_residual = (in_channels == out_channels) & (stride == 1)

        assert stride in (1, 2), "stride should be 1 or 2"
        assert kernel_size in (3, 5), "kernel_size should be 3 or 5"

        hidden_dim = in_channels * expand_ratio
        reduced_dim = np.maximum(1, int(in_channels / reduction_ratio))

        layers = []
        if in_channels != hidden_dim:
            layers.append(ConvBNBlock(in_channels, hidden_dim, kernel_size=1))

        layers.extend([
            ConvBNBlock(hidden_dim, hidden_dim, kernel_size, stride=stride, groups=hidden_dim),
            SqueezeExcitationBlock(hidden_dim, reduced_dim),
            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
        ])
        self.conv = nn.Sequential(*layers)

    def forward(self, x):
        if self.use_residual:
            residual = x
            x = self.conv(x)
            return residual + self._drop_connections(x)
        else:
            return self.conv(x)

    def _drop_connections(self, x):
        """
        dropout probability mask, works similarily as Dropout, except we
        disable individual weights (i.e., set them to zero), instead of nodes
        """
        if not self.training:
            return x  # identity
        keep_probability = 1.0 - self.drop_connect_rate
        batch_size = x.size(0)
        random_tensor = keep_probability + torch.rand(batch_size, 1, 1, 1, device=x.device)
        binary_tensor = random_tensor.floor()
        return x.div(keep_probability) * binary_tensor

# helper functions

def make_divisable(value, divisor=8):
    """ transform input value into closest divisable by divisor value """
    divisable_value = np.maximum(divisor, (value + divisor // 2) // divisor * divisor)
    if divisable_value < 0.9 * value:
        divisable_value += divisor
    return divisable_value

def round_filters(filters, width):
    """ return divisable number of filters """
    if width == 1.0:
        return filters
    return int(make_divisable(filters * width))  # int floor

def round_repeats(repeats, depth):
    """ calibrate number of net blocks """
    if depth == 1.0:
        return repeats
    return int(np.ceil(depth * repeats))

# final model

class EfficientNet(nn.Module):
    """ Gather all blocks (it is possible to upload pretrained weights from torch hub) """
    def __init__(self, *args, width=1.0, depth=1.0, dropout=0.2, num_classes=10, **kwargs):
        super(EfficientNet, self).__init__(*args, **kwargs)
        settings = [
           # t,  c,  n, s, k  -> expand_ratio, channels, repeats, init stride, kernel_size
            [1,  16, 1, 1, 3],  # MBConv1_3x3, SE, 112 -> 112
            [6,  24, 2, 2, 3],  # MBConv6_3x3, SE, 112 ->  56
            [6,  40, 2, 2, 5],  # MBConv6_5x5, SE,  56 ->  28
            [6,  80, 3, 2, 3],  # MBConv6_3x3, SE,  28 ->  14
            [6, 112, 3, 1, 5],  # MBConv6_5x5, SE,  14 ->  14
            [6, 192, 4, 2, 5],  # MBConv6_5x5, SE,  14 ->   7
            [6, 320, 1, 1, 3]   # MBConv6_3x3, SE,   7 ->   7
        ]
        out_channels = round_filters(32, width)
        layers = [ConvBNBlock(3, out_channels, kernel_size=3, stride=2),]

        in_channels = out_channels
        for expand, channel, repeat, strid, kernel in settings:
            out_channels = round_filters(channel, width)
            repeats = round_repeats(repeat, depth)
            for i in range(repeats):
                stride = strid if i == 0 else 1  # reduce spatial dims only on first step
                layers.extend([
                    MBConvBlock(in_channels, out_channels, expand_ratio=expand, kernel_size=kernel, stride=stride)
                ])
                in_channels = out_channels

        last_channels = round_filters(1280, width)
        layers.append(ConvBNBlock(in_channels, last_channels, kernel_size=1))

        self.features = nn.Sequential(*layers)  # name as in torch hub
        self.classifier = nn.Sequential(
                        nn.Dropout(p=dropout),
                        nn.Linear(last_channels, num_classes)
                )

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out")
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                fan_out = m.weight.size(0)
                init_range = 1.0 / np.sqrt(fan_out)
                nn.init.uniform_(m.weight, -init_range, init_range)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x):
        x = self.features(x)
        x = x.mean(dim=[2, 3])  # flatten by mean of spatial dims
        x = self.classifier(x)
        return x

"""#### Dataset"""

directory = "/kaggle/input/isic-2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input/"

data = pd.read_csv("/kaggle/input/isic-2019/ISIC_2019_Training_GroundTruth.csv").drop("UNK", axis=1)
data["labels"] = data.iloc[:, 1:].idxmax(axis=1)
data = data[~data["image"].str.contains("downsampled")]

classes_to_int = {v: i for i, v in enumerate(data.columns[1:-1])}
int_to_classes = {i: v for i, v in enumerate(data.columns[1:-1])}

data["labels"] = data["labels"].map(classes_to_int)

num_classes = len(classes_to_int)

data.head()

data["labels"].value_counts(normalize=True)

# class weights
# random_weighted_sampler for batch composing in dataloader
# focal loss

# assign higher weight for minority classes in cross-entropy loss: loss gets higher when model make mistakes on minor class
class_weights = compute_class_weight("balanced", classes=np.unique(data["labels"]), y=data["labels"])
# class_weights = class_weights / class_weights.sum()  # weights normalization, unneccessary

# augmentations
# train: different random flips, rotations, and color shifts
train_transforms = A.Compose([
                      A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2,
                                                    sat_shift_limit=0.2,
                                                    val_shift_limit=0.2,
                                                    p=0.2),
                      A.RandomBrightnessContrast(brightness_limit=0.2,
                                                 contrast_limit=0.2,
                                                 p=0.5)],p=0.2),
                      A.OneOf(
                              [A.HorizontalFlip(p=0.5),
                               A.VerticalFlip(p=0.5),
                               A.RandomRotate90(p=0.5),
                               A.Transpose(p=0.5),
                              ], p=0.5),
                      A.Resize(height=image_size, width=image_size, p=1),
                      A.Cutout(num_holes=6, max_h_size=10, max_w_size=10, fill_value=0, p=0.1),
                      A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),
                      ToTensorV2(p=1.0),
                      ], p=1.0)

# only resize, scale [-1, 1] and converting to tensor array[h,w,c] -> tensor[c,h,w]
valid_transforms = A.Compose([
                      A.Resize(height=image_size, width=image_size, p=1),
                      A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=255.0),
                      ToTensorV2(p=1.0),
                      ], p=1.0)

# inverse trasformations of a single image-tensor
def inverse_transforms(tensor):
    tensor = tensor
    if tensor.size(0) == 1 and len(tensor.shape) == 4:
        tensor.squeeze_(0)
    tensor = torch.clamp(tensor * 0.5 + 0.5, min=0., max=1.)
    tensor = tensor.cpu().detach().numpy().transpose(1,2,0)

    return tensor

x_train, x_test = train_test_split(data, test_size=1600, stratify=data["labels"], random_state=seed)
x_valid, x_test = train_test_split(x_test, test_size=400, stratify=x_test["labels"], random_state=seed)

x_train.reset_index(drop=True, inplace=True)
x_valid.reset_index(drop=True, inplace=True)
x_test.reset_index(drop=True, inplace=True)

print(f"train size: {len(x_train)}, valid size: {len(x_valid)}, test size: {len(x_test)}")

# define dataset and dataloder

class ISICDataset(Dataset):
    def __init__(self, data, transforms):
        self.data = data
        self.transforms = transforms

    def __len__(self):
        return len(self.data)

    def __getitem__(self, ix):
        row = self.data.loc[ix].squeeze()
        image = Image.open(directory + row["image"] + ".jpg")
        image = np.array(image)

        sample = {"image": image}
        image = self.transforms(**sample)["image"]

        label = torch.as_tensor(row["labels"], dtype=torch.int64)

        return image, label

    def collate_fn(self, batch):
        images, labels = list(zip(*batch))
        images, labels = [[tensor[None] for tensor in subset] for subset in (images, labels)]
        images, labels = [torch.cat(subset, dim=0).to(device) for subset in (images, labels)]
        return images, labels

batch_size = 128

train_ds = ISICDataset(x_train, train_transforms)
valid_ds = ISICDataset(x_valid, valid_transforms)
test_ds = ISICDataset(x_test, valid_transforms)

train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=train_ds.collate_fn)
valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=valid_ds.collate_fn)
test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=test_ds.collate_fn)